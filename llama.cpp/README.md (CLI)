<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Running Models with llama.cpp (CUDA)</title>
  <style>
    body {
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI",
                   Helvetica, Arial, sans-serif;
      line-height: 1.6;
      max-width: 900px;
      margin: 40px auto;
      padding: 0 20px;
      color: #24292f;
    }
    h1, h2, h3 {
      margin-top: 1.5em;
    }
    hr {
      border: none;
      border-top: 1px solid #d0d7de;
      margin: 2em 0;
    }
    code {
      background-color: #f6f8fa;
      padding: 2px 6px;
      border-radius: 4px;
      font-size: 0.95em;
    }
    pre {
      background-color: #f6f8fa;
      padding: 16px;
      border-radius: 6px;
      overflow-x: auto;
    }
    pre code {
      background: none;
      padding: 0;
    }
    blockquote {
      border-left: 4px solid #d0d7de;
      padding-left: 16px;
      color: #57606a;
      margin: 1em 0;
    }
    ul {
      margin-left: 20px;
    }
  </style>
</head>

<body>

<h1>Running Models with <code>llama.cpp</code> (CUDA)</h1>

<p>
This guide explains how to build <strong><code>llama.cpp</code> with CUDA support</strong>,
convert a Hugging Face model to <strong>GGUF</strong>, and run inference using
<code>llama-cli</code>.
</p>

<hr />

<h2>1. Clone the <code>llama.cpp</code> Repository</h2>

<pre><code>git clone https://github.com/ggml-org/llama.cpp.git</code></pre>

<hr />

<h2>2. Navigate to the Repository</h2>

<pre><code>cd llama.cpp</code></pre>

<hr />

<h2>3. Build <code>llama-cli</code> Using CMake (CUDA Enabled)</h2>

<pre><code>mkdir -p build
cd build
cmake .. -DLLAMA_CUDA=ON -DCMAKE_BUILD_TYPE=Release
cmake --build . -j</code></pre>

<blockquote>
  ‚ö†Ô∏è <strong>Note</strong>:<br />
  Use a normal hyphen (<code>-j</code>), not an en dash (<code>‚Äìj</code>),
  or CMake will fail.
</blockquote>

<hr />

<h2>4. Locate the Executable</h2>

<p>After a successful build, <code>llama-cli</code> will be available at:</p>

<pre><code>build/bin/llama-cli</code></pre>

<hr />

<h2>5. Convert a Hugging Face Model to GGUF (If Needed)</h2>

<p>
If the model already provides a <code>.gguf</code> file, you can skip this step.
Otherwise, use the conversion script provided by <code>llama.cpp</code>.
</p>

<h3>Example: Converting <em>Phi-3.5-mini-instruct</em> to FP16 GGUF</h3>

<pre><code>python3 convert_hf_to_gguf.py microsoft/Phi-3.5-mini-instruct \
  --remote \
  --verbose \
  --outfile ./models/Microsoft-phi-3.5-f16 \
  --outtype f16</code></pre>

<blockquote>
  üìÅ Here, <code>models/</code> is the directory where the generated
  <code>.gguf</code> file will be stored.
</blockquote>

<hr />

<h2>6. Run Inference Using <code>llama-cli</code></h2>

<p>
If you have multiple prompts, place them in a text file
(one prompt per line), for example <code>prompts.txt</code>.
</p>

<h3>Example Command</h3>

<pre><code>./build/bin/llama-cli \
  --model &lt;path_to_model.gguf&gt; \
  -f prompts.txt \
  -b 2 \
  -n 200 \
  -c 2048 \
  --perf</code></pre>

<hr />

<h2>Notes on Performance</h2>

<ul>
  <li>This model performs well with the parameters shown above.</li>
  <li>Additional options such as:</li>
  <ul>
    <li><code>--flash-attn</code></li>
    <li><code>--cache-type-k</code></li>
    <li><code>--cache-type-v</code></li>
  </ul>
  <li>
    generally show <strong>negligible throughput differences</strong> for this model.
  </li>
</ul>

<hr />

<h2>References</h2>

<ul>
  <li>
    <code>llama.cpp</code>:
    <a href="https://github.com/ggml-org/llama.cpp" target="_blank">
      https://github.com/ggml-org/llama.cpp
    </a>
  </li>
  <li>GGUF format documentation is available in the <code>llama.cpp</code> repository.</li>
</ul>

</body>
</html>
